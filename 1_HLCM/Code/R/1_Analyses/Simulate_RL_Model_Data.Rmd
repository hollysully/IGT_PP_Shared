---
title: "Simulate RL Model Data"
output: html_document
---


# -------------------------------------------
# Setup
## Load Packages
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(rstan)
library(hBayesDM)
library(bayesplot)
library(here)
library(miscTools)
# library(wBoot)
library(cmdstanr)
library(stringi)
library(ggpp)
library(lemon)
source(here("1_IGT_PP", "Code", "R", "3_other", "helpful_functions.R"))
library(MASS)
library(ggh4x)
library(grid)
```


## Functions
```{r, eval = T}
#------------------------------------------------------------------------------
# USE SOFTMAX TO OBTAIN CHOICE PROBABILITIES - USED IN SIMULATIONS
softmax = function(values){
  return(exp(values) / sum(exp(values)))
}

data = data.frame()

#------------------------------------------------------------------------------
# SIMULATION FUNCTION
simulate_RL = function(learning_rate = .5, n_trials = 100,
                       Sr_probs = c(.3, .7), label = "") {
  A = learning_rate
  utility = rep(0,2)
    
  for(t in 1:n_trials){
    prob = softmax(c(utility[1], utility[2]))
    
    choice = sample(c(1,2), 1, prob = prob)
    
    outcome = rbinom(1, 1, Sr_probs[choice])
    
    data = data.frame(sim = label,
                      A = A,
                      trial = t,
                      prob_1 = prob[1],
                      prob_2 = prob[2],
                      utility_1 = utility[1],
                      utility_2 = utility[2],
                      outcome = outcome,
                      choice = choice) %>% 
      bind_rows(data)
    
    utility[choice] = utility[choice] + A * (outcome - utility[choice])
  }
  data %>%
    return()
}
```


## Test Functions
```{r, eval = T}
set.seed(20240807)
test_data = bind_rows(simulate_RL(learning_rate = .2, label = "hi"),
                     simulate_RL(learning_rate = .05, label = "lo")) %>% 
  pivot_longer(ends_with(c("1", "2")), names_to = "variable", values_to = "value") %>% 
  separate("variable", into = c("variable", "option")) %>% 
  pivot_wider(names_from = "variable", values_from = "value")


ggplot(test_data, aes(x = trial, y = utility, color = option)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(0, 1)) +
  theme_classic() +
  facet_rep_grid(.~sim)


ggplot(test_data, aes(x = trial, y = prob, color = option)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(0, 1)) +
  theme_classic() +
  facet_rep_grid(.~sim)


ggplot(test_data, aes(x = trial, y = utility, color = sim)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(0, 1)) +
  theme_classic() +
  facet_rep_grid(.~option)


ggplot(test_data, aes(x = trial, y = prob, color = sim)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(0, 1)) +
  theme_classic() +
  facet_rep_grid(.~option)
```


# -------------------------------------------
# Simulate Parameters
## Setup
```{r, eval = T}
# CONDITIONS
condition = list(
                 # # small slope effect & weak correlation
                 mu_beta_UT  = c("Lo" = 0),
                 int_slope_R = c("Lo" = 0))
                 
                 # large slope effect & weak correlation
                 # mu_beta_UT  = c("Hi" = .5),
                 # int_slope_R = c("Lo" = 0))
                 
                 # # small slope effect & strong correlation
                 # mu_beta_UT  = c("Lo" = 0),
                 # int_slope_R = c("Hi" = .5))
                 
                 # # large slope effect & strong correlation
                 # mu_beta_UT  = c("Hi" = .5),
                 # int_slope_R = c("Hi" = .5))


# SPECIFY SESSIONS & SAMPLE SIZE
S = 5   # number of sessions
N = 100 # number of participants


# SPECIFY PARAMETERS
mu_intercept_UT = qnorm(.1) # group-level mean untransformed intercept for learning rate
sd_intercept_UT = 1         # group-level standard deviation of intercepts

mu_beta_UT = condition$mu_beta_UT[1][[1]] # group-level slope effect for session
sd_beta_UT = 1  # group-level standard deviation of intercepts

residual_sd = 1 # standard deviation of residuals


# BUILD VARIANCE-COVARIANCE MATRIX
int_slope_R = condition$int_slope_R[1][[1]] # correlation between intercepts and slopes
R = matrix(data = c(1, int_slope_R, int_slope_R, 1), nrow = 2, ncol = 2) # correlation matrix
SD = diag(c(sd_intercept_UT, sd_beta_UT)) # SD matrix
VC = SD%*%R%*%SD # variance-covariance matrix
```


## Simulation
```{r, eval = T}
set.seed(20240611)

# simulate & plot person-level intercept & slope parameters
parameters = mvrnorm(N, mu = c(mu_intercept_UT, mu_beta_UT), Sigma = VC) # draw from MVT Normal
hist(parameters[,1])
hist(parameters[,2])

# combine the parameters into the untransformed As (i.e., on normal distribution scale)
A_UTs = parameters[,1] + parameters[,2]%*%t(1:S-1) + replicate(5, rnorm(N, 0, residual_sd))
As = pnorm(A_UTs)
```


## Inspect Parameters
```{r, eval = T}
# plot untransformed As
plot(rep(1:S, each = N), A_UTs) +
  points(1:S, apply(A_UTs, 2, mean),
         pch = 15, col = "red", cex = 1.5)

# plot transformed As
plot(rep(1:S, each = N), pnorm(A_UTs)) +
  points(1:S, apply(As, 2, mean),
         pch = 15, col = "red", cex = 1.5)

# run MLM to see if it captures changes across time
data.frame(ID = 1:N, session = A_UTs) %>%
  pivot_longer(starts_with("session"), names_to = "session", values_to = "A_UT",
               names_prefix = "session.", names_transform = list(session = as.numeric)) %>%
  mutate(A = pnorm(A_UT), session = session-1) %>%
  lmerTest::lmer(A_UT ~ session + (session|ID), data = .) %>%
  summary()
```


# -------------------------------------------
# Simulate Choice Data
## Generate Data
```{r}
set.seed(20240807)

sim_data = data.frame()

for(i in 1:N){
  for(s in 1:S){
    sim_data = simulate_RL(learning_rate = As[i,s], 
                           label = as.numeric(paste(s, i, sep = "0"))) %>% 
      mutate(ID = i, session = s) %>% 
      bind_rows(sim_data)
  }
}
```


## Convert to Stan Data
```{r}
Tr = max(sim_data$trial)
choice = array(-1, dim = c(N,Tr,S))
outcome = array(0, dim = c(N,Tr,S))

for(s in 1:S){
  for(i in 1:N){
    cur_data = filter(sim_data, session == s, ID == i) %>% 
      arrange(trial)
    choice[i,,s] = cur_data$choice
    outcome[i,,s] = cur_data$outcome
  }
}


stan_data = list(N = N,
                 T = Tr,
                 S = S,
                 choice = choice,
                 outcome = outcome)
```


# -------------------------------------------
# Save Data
```{r}
# PARAMETERS
saveRDS(list(S = S, N = N,
             mu_intercept_UT = mu_intercept_UT,
             sd_intercept_UT = sd_intercept_UT,
             mu_beta_UT = mu_beta_UT,
             sd_beta_UT = sd_beta_UT,
             residual_sd = residual_sd,
             int_slope_R = int_slope_R,
             VC = VC,
             parameters = parameters,
             A_UTs = A_UTs,
             As = As),
        here("1_HLCM", "Data", "0_Simulated_RL",
             paste0("RL_Parameters_", names(condition$mu_beta_UT), "B_",
                    names(condition$int_slope_R), "R.RDS")))


# CHOICE DATA
saveRDS(stan_data, here("1_HLCM", "Data", "1_Stan",
                        paste0("RL_Choices_", names(condition$mu_beta_UT), "B_",
                               names(condition$int_slope_R), "R.RDS")))
```




